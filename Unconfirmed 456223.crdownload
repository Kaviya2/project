# -*- coding: utf-8 -*-
"""Copy of Stress_Detection_07Mar2021_finala.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HOKJImT5j3pfAortxiRdLQvP3yQqFaCN

# Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.express as px
import plotly.graph_objects as go
import warnings
warnings.filterwarnings('ignore')
import string

"""# dataset

#  Read the File through Pandas Library
"""

y1 = pd.read_csv("data.csv")
display(y1.shape)
display(y1.head())
display("")
display(y1.tail())

"""# Information about Dataset"""

def display_file_info(df,df_name):
        col_name_list      = list(df.columns)
        col_data_type      = [type(col) for col in df.iloc[0,:]]
        null_count_list    = [df[col].isnull().sum() for col in col_name_list]
        unique_count_list  = [df[col].nunique() for col in col_name_list]
        memory_usage_list  = [df[col].memory_usage() for col in col_name_list]
        total_memory_usage = sum(memory_usage_list)
        df_info = pd.DataFrame({'column_name': col_name_list,
                               'column_type': col_data_type,
                               'null_count': null_count_list,
                               'unique_count':unique_count_list})
        print('-------- {} Summary --- {} x rows | {} x cols |approx {:.2f} MB:'
                                          .format(df_name , df.shape[0], df.shape[1],total_memory_usage))
        print( df_info.to_string())

########

display_file_info(y1,'Input Data')

" there is no - null values"

y1.drop_duplicates(inplace = True)
y1.shape

"""# index/column name"""

a = y1.columns
a

"""# Target Class

# 1 -- stress present
# 0 -- not-stress
"""

predict_class_count = y1.target.value_counts()
display(predict_class_count)

from plotly.offline import iplot
pred_class = ['Stress', 'Not Stress' ]
trace = go.Pie(labels = pred_class, values = predict_class_count)
data = [trace]
fig = go.Figure(data = trace)
iplot(fig)

"""#  Statistical details of Dataframe"""

y1.describe()

"""# Data Visualz

# Sex/Gender 
#  1 - Male , 0 - Frmale
"""

sns.pairplot(data=y1,hue='sex')

"""# numpy library to calculate the standard deviation for the ages of male and female"""

sns.barplot(x='sex', y='age', data=y1, estimator=np.std)

"""# Gender
# 1 - Male, 0 - Female
"""

sns.countplot(x='sex', data=y1)

sns.catplot(x="sex", y="age", hue="target",
            kind="violin", data=y1)

sns.catplot(x="sex", y="age", hue="target",
            data=y1)

"""# Correlation"""

correlatee = y1.corr()
correlatee

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sns.set(rc={'figure.figsize':(15,15)})
sns.heatmap(correlatee,annot=True,vmin=-1,vmax=1,cmap='coolwarm')

"""# Calling Classifier"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV as GSCV
from sklearn.model_selection import cross_validate as CV
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.naive_bayes import GaussianNB as NBC
from sklearn.linear_model import LogisticRegression as LR
from sklearn.linear_model import SGDClassifier as SDC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score

Xa_1 = y1.iloc[:,:-1].values
Target_Class = y1.iloc[:,-1].values

Xa_1.shape
Target_Class.shape

Target_Class

"""# data split for training & testing"""

X_train, X_test, y_train, y_test = train_test_split(Xa_1, Target_Class, random_state=0 , test_size=0.30,stratify=Target_Class)
display("for training")
print(X_train.shape, y_train.shape)
display("for testing")
print(X_test.shape, y_test.shape)

"""# Scale the Data"""

from sklearn.preprocessing import StandardScaler as SC
std_scaler = SC()

X_train_1 = std_scaler.fit_transform(X_train)
X_test_1 = std_scaler.transform(X_test)

"""# creating an empty list to save the results (classifier)"""

model_names = []
feature_counts = []
model_acc_scores = []
model_bac_scores = []
model_prc_scores = []
model_rec_scores = []
model_f1_scores = []

"""# calling multiple classifier"""

# RFC -- Model 1
rfc_model_1 = RFC(random_state=0)
rfc_model_1.fit(X_train, y_train)
rfc_model_1_y_hat = rfc_model_1.predict(X_test)
model_names.append('rfc_model_1')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, rfc_model_1_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, rfc_model_1_y_hat))
model_prc_scores.append(precision_score(y_test, rfc_model_1_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, rfc_model_1_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, rfc_model_1_y_hat, average='micro'))

# RFC -- Model 2
rfc_model_2 = RFC(random_state = 1, class_weight='balanced')
rfc_model_2.fit(X_train, y_train)
rfc_model_2_y_hat = rfc_model_2.predict(X_test)
model_names.append('rfc_model_2')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, rfc_model_2_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, rfc_model_2_y_hat))
model_prc_scores.append(precision_score(y_test, rfc_model_2_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, rfc_model_2_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, rfc_model_2_y_hat, average='micro'))

# DTC Model
dtc_model_1 = DTC(random_state=0)
dtc_model_1.fit(X_train, y_train)
dtc_model_1_y_hat = dtc_model_1.predict(X_test)
model_names.append('dtc_model_1')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, dtc_model_1_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, dtc_model_1_y_hat))
model_prc_scores.append(precision_score(y_test, dtc_model_1_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, dtc_model_1_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, dtc_model_1_y_hat, average='micro'))

# Navie Bayes
nbc_model_1 = NBC()
nbc_model_1.fit(X_train, y_train)
nbc_model_1_y_hat = nbc_model_1.predict(X_test)
model_names.append('nbc_model_1')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, nbc_model_1_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, nbc_model_1_y_hat))
model_prc_scores.append(precision_score(y_test, nbc_model_1_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, nbc_model_1_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, nbc_model_1_y_hat, average='micro'))
# Support vector
svc_model_1 = SVC(random_state=0)
svc_model_1.fit(X_train, y_train)
svc_model_1_y_hat = svc_model_1.predict(X_test)
model_names.append('svc_model_1')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, svc_model_1_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, svc_model_1_y_hat))
model_prc_scores.append(precision_score(y_test, svc_model_1_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, svc_model_1_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, svc_model_1_y_hat, average='micro'))
# kNN
knc_model_1 = KNN()
knc_model_1.fit(X_train, y_train)
knc_model_1_y_hat = knc_model_1.predict(X_test)
model_names.append('knc_model_1')
feature_counts.append(X_train.shape[1])
model_acc_scores.append(accuracy_score(y_test, knc_model_1_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test, knc_model_1_y_hat))
model_prc_scores.append(precision_score(y_test, knc_model_1_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test, knc_model_1_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test, knc_model_1_y_hat, average='micro'))

display(y_test,rfc_model_1_y_hat)

df_model_eval = pd.DataFrame({'model': model_names, 'feature_count': feature_counts, 
                                    'acc': model_acc_scores, 'bac': model_bac_scores, 
                                    'prc': model_prc_scores, 'rec': model_rec_scores, 
                                    'f1': model_f1_scores})
df_model_eval

"""# RF(Random Forest) Feature Selection"""

XX =   y1.iloc[:,:-1] 
yy =  y1.iloc[:,-1]

yy

rfc_model_1.fit(XX,yy)

feature_importances = pd.DataFrame({'feature': list(XX.columns), 
                                    'importance': list(rfc_model_1.feature_importances_)})
feature_importances.sort_values(by='importance', inplace=True)
feature_importances.set_index('feature', drop=True, inplace=True)
feature_importances.plot(kind='barh')

XX.head(2)

"""# removing less priority features"""

df_train_features = XX.drop(columns = ['fbs', 'restecg','sex'])
df_train_features.head(3)

"""# train test split -- new"""

X_train1, X_test1, y_train1, y_test1 = train_test_split(df_train_features, Target_Class , test_size=0.30)
display("for training")
print(X_train1.shape, y_train1.shape)
display("for testing")
print(X_test1.shape, y_test1.shape)

# RFC -- Model 1
rfc_model_1a = RFC()
rfc_model_1a.fit(X_train1, y_train1)
rfc_model_1a_y_hat = rfc_model_1a.predict(X_test1)
model_names.append('rfc_model_1a')
feature_counts.append(X_train1.shape[1])
model_acc_scores.append(accuracy_score(y_test1, rfc_model_1a_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test1, rfc_model_1a_y_hat))
model_prc_scores.append(precision_score(y_test1, rfc_model_1a_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test1, rfc_model_1a_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test1, rfc_model_1a_y_hat, average='micro'))

# RFC -- Model 2
rfc_model_2a = RFC(random_state=0 , class_weight='balanced')
rfc_model_2a.fit(X_train1, y_train1)
rfc_model_2a_y_hat = rfc_model_2a.predict(X_test1)
model_names.append('rfc_model_2a')
feature_counts.append(X_train1.shape[1])
model_acc_scores.append(accuracy_score(y_test1, rfc_model_2a_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test1, rfc_model_2a_y_hat))
model_prc_scores.append(precision_score(y_test1, rfc_model_2a_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test1, rfc_model_2a_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test1, rfc_model_2a_y_hat, average='micro'))

# DTC Model
dtc_model_1a = DTC(random_state=0)
dtc_model_1a.fit(X_train1, y_train1)
dtc_model_1a_y_hat = dtc_model_1a.predict(X_test1)
model_names.append('dtc_model_1a')
feature_counts.append(X_train1.shape[1])
model_acc_scores.append(accuracy_score(y_test1, dtc_model_1a_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test1, dtc_model_1a_y_hat))
model_prc_scores.append(precision_score(y_test1, dtc_model_1a_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test1, dtc_model_1a_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test1, dtc_model_1a_y_hat, average='micro'))

# Navie Bayes
nbc_model_1a = NBC()
nbc_model_1a.fit(X_train1, y_train1)
nbc_model_1a_y_hat = nbc_model_1a.predict(X_test1)
model_names.append('nbc_model_1a')
feature_counts.append(X_train1.shape[1])
model_acc_scores.append(accuracy_score(y_test1, nbc_model_1a_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test1, nbc_model_1a_y_hat))
model_prc_scores.append(precision_score(y_test1, nbc_model_1a_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test1, nbc_model_1a_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test1, nbc_model_1a_y_hat, average='micro'))
# Support vector
svc_model_1a = SVC(random_state=0)
svc_model_1a.fit(X_train1, y_train1)
svc_model_1a_y_hat = svc_model_1a.predict(X_test1)
model_names.append('svc_model_1a')
feature_counts.append(X_train1.shape[1])
model_acc_scores.append(accuracy_score(y_test1, svc_model_1a_y_hat))
model_bac_scores.append(balanced_accuracy_score(y_test1, svc_model_1a_y_hat))
model_prc_scores.append(precision_score(y_test1, svc_model_1a_y_hat, average='micro'))
model_rec_scores.append(recall_score(y_test1, svc_model_1a_y_hat, average='micro'))
model_f1_scores.append(f1_score(y_test1, svc_model_1a_y_hat, average='micro'))

df_model_eval = pd.DataFrame({'model': model_names, 'feature_count': feature_counts, 
                                    'acc': model_acc_scores, 'bac': model_bac_scores, 
                                    'prc': model_prc_scores, 'rec': model_rec_scores, 
                                    'f1': model_f1_scores})
df_model_eval

"""# when compare the classifier results, "nbc_model_1 - Naive Bayes classiier has good accuracy""""

df_predict = pd.DataFrame({'actual result': y_test, 'predicted result': nbc_model_1_y_hat}) 

df_predict

"""# confusion_matrix"""

from sklearn.metrics import confusion_matrix
cf_matrix=confusion_matrix(y_test,nbc_model_1a_y_hat)
print(cf_matrix)
sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, 
            fmt='.2%', cmap='Blues')

